# 深度学习课程基础作业二

## 一、课程核心概念深入研究

### （一）卷积层（Convolutional Layer）

卷积层是卷积神经网络（CNN）的核心组件，其设计灵感来源于生物视觉系统的局部感受野机制，主要作用是从输入数据（如图像）中提取局部特征（如边缘、纹理、形状等）。

#### 1. 核心原理

卷积层通过**卷积运算**实现特征提取，核心元素包括：

*   **卷积核（Kernel）**：又称滤波器，是一个小型矩阵（如 3×3、5×5），其数值为可学习的参数。每个卷积核对应一种特征模式（如垂直边缘、水平边缘），通过与输入数据的局部区域相乘累加，检测该区域是否存在对应特征。

*   **步长（Stride）**：卷积核在输入数据上移动的步幅（通常为 1 或 2）。步长越大，输出特征图尺寸越小，计算效率越高，但可能丢失细节。

*   **填充（Padding）**：在输入数据边缘填充 0，用于保持输出特征图与输入尺寸一致（如 “same padding”），或减少边缘信息丢失。

#### 2. 计算方式

单个特征图的输出尺寸计算公式为：

`输出尺寸 = [(输入尺寸 - 卷积核尺寸) / 步长] + 1`

例如：输入图像尺寸为 28×28，卷积核尺寸 3×3，步长 1，无填充时，输出特征图尺寸为 26×26。

#### 3. 特征提取逻辑

卷积层通过多个卷积核并行运算，生成多通道特征图（输出通道数 = 卷积核数量）。浅层卷积层通常提取边缘、颜色等低级特征，深层卷积层则组合低级特征，形成纹理、部件等高级特征（如车辆的车轮、车身轮廓）。这种 “局部感知 + 权值共享” 的特性，既减少了参数数量（相比全连接层），又增强了对平移不变性的适应能力（如图像中物体位置偏移仍可被识别）。

### （二）反向传播算法（Backpropagation）

反向传播算法是训练神经网络的核心方法，其本质是通过**链式法则**计算损失函数对各层参数的梯度，再利用梯度下降更新参数，最终最小化损失。

#### 1. 与梯度下降的关系

梯度下降是 “优化目标”（通过参数更新降低损失），反向传播是 “实现手段”（高效计算梯度）。没有反向传播时，直接计算深层网络的梯度会因参数数量庞大而不可行。

#### 2. 工作流程



*   **前向传播**：输入数据通过网络各层计算，得到预测值，再与真实值计算损失（如交叉熵损失）。

*   **反向传播**：从输出层开始，根据链式法则逐层计算损失对参数的梯度（如卷积核权重、偏置）。例如，输出层的梯度直接由损失函数对输出的导数计算，隐藏层的梯度则由后一层的梯度与当前层的激活函数导数相乘得到。

*   **参数更新**：根据梯度（如`w = w - learning_rate × ∇w`）调整各层参数，重复前向传播和反向传播，直到损失收敛。

#### 3. 关键意义

反向传播解决了深层网络的训练难题，使得多层神经网络（深度学习）从理论走向实践。其效率来自于 “梯度复用”—— 每层的梯度计算仅依赖后一层的梯度，避免了重复计算，大幅降低了复杂度。

## 二、课程外深度学习知识拓展

### （一）高级数据增强方法

除了 PPT 中提到的`transforms`基础变换（如 Resize、Normalize），实际训练中常用以下增强策略提升模型泛化能力：



*   **Mixup**：将两张图像按比例混合（如`image = α×image1 + (1-α)×image2`），标签也对应混合，增强模型对样本间关联性的学习。

*   **Cutmix**：随机裁剪一张图像的部分区域，粘贴到另一张图像上，标签按裁剪区域比例分配，既保留局部特征，又避免信息丢失。

*   **RandAugment**：通过随机选择旋转、缩放、色彩抖动等操作的组合，自动调整增强强度，减少人工调参成本。

### （二）注意力机制（Attention Mechanism）

注意力机制模拟人类视觉的 “聚焦” 特性，使网络在处理数据时重点关注关键区域（如图像中的目标、文本中的关键词）。在 CNN 中，常见的注意力模块（如 SE-Net）通过学习 “通道权重”，增强重要特征通道的贡献，抑制无关通道，提升模型性能。例如，在锥桶识别任务中，注意力机制可让网络更关注锥桶的颜色和形状区域，而非背景。

## 三、对深度学习的认知

经过本次学习，我对深度学习的认知可总结为以下三点：

1.  PPT 中强调 “数据集准备非常重要”，实际训练中，数据的质量（如标注准确性）和数量直接决定模型上限。即使是简单的网络，若有高质量数据集，也能取得不错的效果；反之，复杂网络在劣质数据上可能过拟合。

2.  不同任务需要匹配不同网络结构。例如，CNN 适合处理图像（利用局部相关性），RNN 适合序列数据（如文本、语音），而 Transformer 凭借注意力机制在两者上均有突破。在锥桶分类任务中，CNN 的卷积层能有效提取颜色和形状特征，是更优选择。

3.  训练过程中需平衡过拟合与欠拟合 —— 通过正则化（如 Dropout）、数据增强防止过拟合；通过加深网络、调整学习率解决欠拟合。同时，损失函数与优化器的选择也需匹配任务（如分类用交叉熵，回归用 MSE；Adam 在多数场景下收敛更快，但 SGD 可能泛化更好）。

## 四、ResNet 论文解读

### （一）背景与问题

在 ResNet 提出前，研究者发现当网络深度增加时，模型性能会先提升后下降（“退化问题”），并非过拟合导致，而是深层网络的梯度在反向传播中易消失或爆炸，导致参数难以更新。

### （二）核心创新：残差连接（Residual Connection）

ResNet 的核心是**残差块（Residual Block）**，其结构如下：

`输出 = 输入 + F(输入)`

其中，`F(输入)`是 “残差函数”（由卷积层、激活函数等组成），表示输入与输出的差异。



*   **作用**：若深层网络已达到最优，残差函数`F(输入)`可学习为 0，此时网络等价于浅层网络，避免性能退化。同时，残差连接使梯度能直接从输出层传播到浅层，缓解梯度消失问题。

### （三）网络结构

ResNet 有多个版本（如 18 层、34 层、50 层、101 层、152 层），层数差异主要来自残差块的数量。以 ResNet-50 为例：

*   采用 “bottleneck 残差块”：由 1×1 卷积（降维）、3×3 卷积（特征提取）、1×1 卷积（升维）组成，减少参数数量。

*   整体结构：输入层→4 个卷积阶段（每个阶段包含多个残差块）→全局平均池化→全连接层（分类）。

### （四）性能与影响

ResNet 在 2015 年 ImageNet 竞赛中夺冠，错误率远低于同期模型。其残差连接思想被广泛应用于后续网络（如 ResNeXt、DenseNet），成为深度学习的基础组件。在锥桶分类任务中，使用 ResNet-18 可通过深层特征提取提升小样本下的分类精度，同时残差连接避免了浅层网络特征不足的问题。